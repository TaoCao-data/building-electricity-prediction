{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Demo of running the notebook with full code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import all modules and functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "toggle_code_str = '''\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Source Code\"></form>\n",
    "'''\n",
    "\n",
    "toggle_code_prepare_str = '''\n",
    "    <script>\n",
    "    function code_toggle() {\n",
    "        if ($('div.cell.code_cell.rendered.selected div.input').css('display')!='none'){\n",
    "            $('div.cell.code_cell.rendered.selected div.input').hide();\n",
    "        } else {\n",
    "            $('div.cell.code_cell.rendered.selected div.input').show();\n",
    "        }\n",
    "    }\n",
    "    </script>\n",
    "\n",
    "'''\n",
    "\n",
    "display(HTML(toggle_code_prepare_str + toggle_code_str))\n",
    "\n",
    "def toggle_code():\n",
    "    display(HTML(toggle_code_str))\n",
    "\n",
    "# import modules\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "from sklearn import base\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "\n",
    "#%%\n",
    "# model score function\n",
    "def model_score_rmse(Y):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : dataframe\n",
    "        with timestamp as index, column 0 as true values, and column 1 as \n",
    "        predicted values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float value\n",
    "        RMSE.\n",
    "    '''\n",
    "    \n",
    "    metric_rmse = np.sqrt(mean_squared_error(Y[Y.columns[0]], Y[Y.columns[1]]))\n",
    "    \n",
    "    metric_cv_rmse = (metric_rmse / Y[Y.columns[0]].mean()) * 100\n",
    "    \n",
    "    return metric_rmse, metric_cv_rmse\n",
    "\n",
    "# meter reading comparison plot\n",
    "def plot_pred(Y):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : dataframe\n",
    "        timestamp as index, first column of true y, 2nd column of predicted y.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A figure of time series plot\n",
    "\n",
    "    '''\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d, %A'))\n",
    "    ax.xaxis.set_major_locator(mdates.DayLocator(interval = 1))\n",
    "    \n",
    "    ax.plot(Y.index, Y[Y.columns[0]], label = 'true')\n",
    "    ax.plot(Y.index, Y[Y.columns[1]], label = 'predicted')\n",
    "    \n",
    "    # rotate and align the tick labels so they look better\n",
    "    fig.autofmt_xdate()\n",
    "    \n",
    "    plt.xlabel('month-date')\n",
    "    plt.ylabel('hourly electricity load (kWh)')\n",
    "    plt.legend(title='load profile type')\n",
    "\n",
    "def create_time_steps(length):\n",
    "    \n",
    "    return list(range(-length, 0))\n",
    "\n",
    "def multi_step_plot(hist, true_future, prediction, baseline):\n",
    "  \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = create_time_steps(len(hist))\n",
    "    num_out = len(true_future)\n",
    "    \n",
    "    plt.plot(num_in, np.expm1(hist), label='History')\n",
    "    plt.plot(np.arange(num_out), np.expm1(true_future), 'g-',\n",
    "             label='True Future')\n",
    "    \n",
    "    if prediction.any():\n",
    "      plt.plot(np.arange(num_out), np.expm1(prediction), 'ro',\n",
    "               label='Predicted Future (Proposed)')\n",
    "    \n",
    "    if baseline.any():\n",
    "      plt.plot(np.arange(num_out), np.expm1(baseline), 'bo',\n",
    "               label='Predicted Future (Baseline)')\n",
    "    \n",
    "    plt.legend(loc='upper left', prop = {'size': 14})\n",
    "    plt.xlabel('time (hour)')\n",
    "    plt.ylabel('hourly electricity load (kWh)')\n",
    "    plt.show()\n",
    "\n",
    "# add time features\n",
    "def time_feature(X):\n",
    "        ''' add time features to X, with timestamps as index '''\n",
    "        \n",
    "        X['month'] = X.index.to_series().apply(lambda x: x.month)\n",
    "        X['week'] = X.index.to_series().apply(lambda x: x.week)\n",
    "        X['day'] = X.index.to_series().apply(lambda x: x.dayofweek)\n",
    "        X['hour'] = X.index.to_series().apply(lambda x: x.hour)\n",
    "        \n",
    "        # Cyclic encoding of periodic features\n",
    "        X['hour_x'] = np.cos(2*np.pi*X['hour']/24)\n",
    "        X['hour_y'] = np.sin(2*np.pi*X['hour']/24)\n",
    "        \n",
    "        return X\n",
    "\n",
    "class X_preprocessor(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    # count missing values\n",
    "    def count_miss(self, X):\n",
    "        ''' return a dataframe of missing counts and percentage\n",
    "        for each feature'''\n",
    "    \n",
    "        total = X.isnull().sum().sort_values(ascending=False)\n",
    "        ratio = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n",
    "        missing_data = pd.concat([total, ratio], axis=1, keys=['Total', 'Ratio'])\n",
    "        \n",
    "        return missing_data\n",
    "    \n",
    "    # linear interpolate of missing values\n",
    "    def linear_interpolate_miss(self, X, lst):\n",
    "        ''' linear interpolate missing features given in lst '''\n",
    "        \n",
    "        for ele in lst:\n",
    "            X[ele] = X[ele].interpolate(method='linear', limit_direction='forward', axis=0)\n",
    "        \n",
    "        return X      \n",
    "    \n",
    "    # add time features\n",
    "    def time_feature(self, X):\n",
    "        ''' add time features to X, with timestamps as index '''\n",
    "        \n",
    "        X['month'] = X.index.to_series().apply(lambda x: x.month)\n",
    "        X['week'] = X.index.to_series().apply(lambda x: x.week)\n",
    "        X['day'] = X.index.to_series().apply(lambda x: x.dayofweek)\n",
    "        X['hour'] = X.index.to_series().apply(lambda x: x.hour)\n",
    "        \n",
    "        # Cyclic encoding of periodic features\n",
    "        X['hour_x'] = np.cos(2*np.pi*X['hour']/24)\n",
    "        X['hour_y'] = np.sin(2*np.pi*X['hour']/24)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Return an array with the same number of rows as X and transformed columns\n",
    "        \n",
    "        # drop not needed columns\n",
    "        X = X.drop(['meter'], axis = 1)\n",
    "        # sort by building id, then timestamp, ready for missing value handling\n",
    "        X = X.sort_values(by = ['building_id', 'timestamp'])\n",
    "        \n",
    "        # check missing stats\n",
    "        missing_data = self.count_miss(X)\n",
    "\n",
    "        # remove high missing percentage features (precip_depth_1_hr)\n",
    "        X = X.drop(['precip_depth_1_hr'], axis = 1)\n",
    "        \n",
    "        # remove high missing percentage features (cloud_coverage)\n",
    "        X = X.drop(['cloud_coverage'], axis = 1)\n",
    "        \n",
    "        # replace with median (sea_level_pressure)\n",
    "        X['sea_level_pressure'] = X['sea_level_pressure']. \\\n",
    "            apply(pd.to_numeric, errors='coerce').fillna(X['sea_level_pressure'].median())\n",
    "        \n",
    "        # linear interpolation of all missing features (< 1.7%)\n",
    "        lst = list(missing_data.index.values[3:7])\n",
    "        X = self.linear_interpolate_miss(X, lst)\n",
    "        \n",
    "        \n",
    "        # add time features\n",
    "        X = self.time_feature(X)\n",
    "        \n",
    "        # dummy values for categorical features\n",
    "        cat_features = ['site_id', 'primary_use']\n",
    "        X = pd.get_dummies(X, columns = cat_features)\n",
    "        \n",
    "        return X\n",
    "\n",
    "class X_num_scaler(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, num_cols, scaler):\n",
    "        self.num_cols = num_cols  # We will need these in transform()\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # scaler fit to numerical columns\n",
    "        \n",
    "        # cat and num dataframe\n",
    "        num = X[self.num_cols]\n",
    "                \n",
    "        return self.scaler.fit(num)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Return an array with the same number of rows as X and scaled\n",
    "        # numerical columns, unchanged categorical columns\n",
    "        \n",
    "        # cat and num dataframe\n",
    "        num = X[self.num_cols]\n",
    "        cat = X.drop([self.num_cols[i] for i in range(len(self.num_cols))], axis = 1)\n",
    "        \n",
    "        # transform numerical features\n",
    "        num_scaled = pd.DataFrame(self.scaler.transform(num)).set_index(num.index)\n",
    "        num_scaled.columns = num.columns\n",
    "        \n",
    "        # concat back with cat features\n",
    "        X_scaled = pd.concat([num_scaled, cat], axis=1)\n",
    "        \n",
    "        return X_scaled\n",
    "\n",
    "# Series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\t\"\"\"\n",
    "\tFrame a time series as a supervised learning dataset.\n",
    "\tArguments:\n",
    "\t\tdata: Sequence of observations as a list or NumPy array.\n",
    "\t\tn_in: Number of lag observations as input (X).\n",
    "\t\tn_out: Number of observations as output (y).\n",
    "\t\tdropnan: Boolean whether or not to drop rows with NaN values.\n",
    "\tReturns:\n",
    "\t\tPandas DataFrame of series framed for supervised learning.\n",
    "\t\"\"\"\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "    \n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        \n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t\n",
    "    # put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t\n",
    "    # drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "    \n",
    "\treturn agg\n",
    "\n",
    "# plot model convergence\n",
    "def plot_train_history(history, title):\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  epochs = range(len(loss))\n",
    "\n",
    "  plt.figure()\n",
    "\n",
    "  plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "  plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "  plt.title(title)\n",
    "  plt.legend()\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def create_Y_triplets(df_test_reframed, y_test, y_test_hat, n_features, n_lag_hours, i):\n",
    "    \n",
    "    hist = test_reframed.values[i, [n_features * k for k in range(n_lag_hours)]]\n",
    "    truth = np.concatenate( y_test, axis=0 )\n",
    "    pred = np.concatenate( y_test_hat, axis=0 )\n",
    "    \n",
    "    return hist, truth, pred\n",
    "\n",
    "def baseline_predict(df_train, df_test, n_lag_hours, d_pre_days, hist_reps):\n",
    "    \n",
    "    base = []\n",
    "    \n",
    "    t_pred = df_test.iloc[n_lag_hours : n_lag_hours + d_pre_days * 24, :].index\n",
    "    \n",
    "    for i in range(hist_reps):\n",
    "        \n",
    "        t_baseline_hist = t_pred - pd.to_timedelta( 7 * (i + 1), unit = 'days')\n",
    "        \n",
    "        base.append (df_train.loc[t_baseline_hist]['meter_reading'].values)\n",
    "\n",
    "    return np.mean(np.array(base), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defind your inputs**\n",
    "* 1. d_lag: past days used for prediction\n",
    "* 2. d_pre: days (or hours, if it is less than 1) the model predicts. Suggested values are *1 hour or 3 hours*. Default is 3 hours.\n",
    "* 3. d_pre_days: days you would like to predict. Suggested values are *1 day, 2 day or 3 days*. Default is 1 day.\n",
    "* 4. raw_data: sample raw data (pickle file) --> 1 year electric load files of one building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# model settings (CHECK THESE EVERYTIME RUN THE PROGRAM)\n",
    "\n",
    "d_lag = 5\n",
    "\n",
    "d_pre = 3/24\n",
    "\n",
    "d_pre_days = 1\n",
    "\n",
    "df_bldg = pd.read_pickle(\"./raw_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A peek into the raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle_code()\n",
    "\n",
    "df_bldg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A further investigation of the load profiles:**\n",
    "\n",
    "Lots of zeros while high meter readings, a natural way is log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle_code()\n",
    "\n",
    "#%%\n",
    "# Y values investigation\n",
    "X = df_bldg.drop(['meter_reading'], axis = 1)\n",
    "Y = df_bldg.loc[:, ['meter_reading']]\n",
    "\n",
    "# stats\n",
    "print(Y.describe())\n",
    "\n",
    "# target transformations\n",
    "Y['meter_reading_log_transformed'] = np.log1p(Y['meter_reading'])\n",
    "\n",
    "# distplot and boxplot for outlier detections\n",
    "sns.boxplot(Y['meter_reading_log_transformed'], showfliers = True)    \n",
    "\n",
    "# find outliers\n",
    "outliers = [y for stat in boxplot_stats(Y['meter_reading_log_transformed']) for y in stat['fliers']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the data into train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle_code()\n",
    "\n",
    "patience_turn = 60\n",
    "\n",
    "# define days used for training\n",
    "n_train_days = 280\n",
    "\n",
    "# define historic hours used for predictions\n",
    "n_lag_hours = int(24 * d_lag)\n",
    "\n",
    "# define hours in the future for predictions\n",
    "n_pre_hours = int(24 * d_pre)\n",
    "\n",
    "# training hours based on days for training\n",
    "n_train_hours = 24 * n_train_days\n",
    "\n",
    "#%%\n",
    "# train & test split\n",
    "\n",
    "train = df_bldg.iloc[:n_train_hours, :]\n",
    "test = df_bldg.iloc[n_train_hours-n_lag_hours:, :]\n",
    "\n",
    "train_X = train.drop(['meter_reading'], axis = 1)\n",
    "train_Y = train.loc[:, ['meter_reading']]\n",
    "\n",
    "test_X = test.drop(['meter_reading'], axis = 1)\n",
    "test_Y = test.loc[:, ['meter_reading']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess train and test set:**\n",
    "* Handling missing values\n",
    "* Dealing with outliers\n",
    "* Feature engineering\n",
    "* Normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle_code()\n",
    "\n",
    "#%%\n",
    "# train set preparation\n",
    "\n",
    "# initialize preprocessor\n",
    "preprocessor = X_preprocessor()\n",
    "# preprocess X_train\n",
    "train_X_preprocessed = preprocessor.transform(train_X)\n",
    "\n",
    "# initialize scaler\n",
    "num_cols = list(train_X_preprocessed.columns[:15])\n",
    "scaler = X_num_scaler(num_cols, StandardScaler())\n",
    "\n",
    "# fit and transform X_train\n",
    "scaler.fit(train_X_preprocessed)\n",
    "train_X_scaled = scaler.transform(train_X_preprocessed)\n",
    "\n",
    "# log transform of Y_train\n",
    "train_Y_scaled = np.log1p(train_Y)\n",
    "\n",
    "# concat for transformed and scaled train set\n",
    "train_scaled = pd.concat([train_Y_scaled, train_X_scaled], axis = 1)\n",
    "\n",
    "\n",
    "#%%\n",
    "# test set preparation\n",
    "\n",
    "# initialize preprocessor\n",
    "preprocessor = X_preprocessor()\n",
    "# preprocess X_test\n",
    "test_X_preprocessed = preprocessor.transform(test_X)\n",
    "\n",
    "# initialize scaler\n",
    "num_cols = list(test_X_preprocessed.columns[:15])\n",
    "# transform X_test\n",
    "test_X_scaled = scaler.transform(test_X_preprocessed)\n",
    "\n",
    "# log transform of Y_test\n",
    "test_Y_scaled = np.log1p(test_Y)\n",
    "\n",
    "# concat for transformed and scaled train set\n",
    "test_scaled = pd.concat([test_Y_scaled, test_X_scaled], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time to feed the data into the pipeline**\n",
    "\n",
    "* First, set up the hyperparameter search with Keras-tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle_code()\n",
    "\n",
    "#%%\n",
    "# reframe train set\n",
    "\n",
    "n_features = train_scaled.shape[1]\n",
    "\n",
    "train_reframed = series_to_supervised(train_scaled, \n",
    "                                      n_lag_hours, n_pre_hours)\n",
    "\n",
    "n_obs = n_lag_hours * n_features\n",
    "\n",
    "# split into input and outputs\n",
    "\n",
    "x_train, y_train = train_reframed.values[:, :n_obs], \\\n",
    "    train_reframed.values[:, [-n_features * (n_pre_hours - i) for i in range(n_pre_hours)]]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "x_train = x_train.reshape((x_train.shape[0], n_lag_hours, n_features))\n",
    "\n",
    "# print(x_train.shape, y_train.shape)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model(hp):\n",
    "     \n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(units=hp.Int('units', min_value=32, max_value=128, step=16), \n",
    "                                   return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.LSTM(units=hp.Int('units', min_value=16, max_value=96, step=16)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(n_pre_hours))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='mse', optimizer = tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])))\n",
    "    \n",
    "    return model\n",
    "\n",
    "import kerastuner\n",
    "\n",
    "project_path = '{}d_{}d'.format(d_lag, d_pre)\n",
    "\n",
    "tuner = kerastuner.tuners.Hyperband(build_model, objective='val_loss', max_epochs = 1000, \n",
    "                                    directory='hypertuned', project_name = project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A summary of hyperparameter search (based on which we will finalize our LSTM model structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toggle_code()\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=1000, batch_size=32, validation_split=0.2, shuffle=False, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What's the best model looking like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toggle_code()\n",
    "\n",
    "models = tuner.get_best_models(num_models=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since Keras tuner only did a preliminary parameter search, we will continue fit the best model with train dataset.\n",
    "\n",
    "    The train and validation loss plot shows the model learing progress. It also helps to indicate when to stop to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle_code()\n",
    "\n",
    "model = models[0]\n",
    "\n",
    "best_model_filepath = 'best_model_{}d_to_{}d'.format(d_lag, d_pre)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = patience_turn)\n",
    "\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(filepath = best_model_filepath, monitor='val_loss', \n",
    "                             mode='min', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=400, batch_size=32, validation_split=0.2, shuffle=False, \n",
    "                    verbose = 2, callbacks = [es, mc])\n",
    "\n",
    "plot_train_history(history, 'Training and Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the best model we have trained and use it to predict the future electricity loads (within your selected time frame).**\n",
    "\n",
    "Default is 1 days (see d_pre_days)\n",
    "\n",
    "A peek into the load predcition results (first several hours):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle_code()\n",
    "\n",
    "#%%\n",
    "# reframe test set\n",
    "test_reframed = series_to_supervised(test_scaled, \n",
    "                                     n_lag_hours, n_pre_hours)\n",
    "\n",
    "# split into input and outputs\n",
    "x_test, y_test = test_reframed.values[:, :n_obs], \\\n",
    "    test_reframed.values[:, [-n_features * (n_pre_hours - i) for i in range(n_pre_hours)]]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "x_test = x_test.reshape((x_test.shape[0], n_lag_hours, n_features))\n",
    "\n",
    "best_model_filepath_2 = 'best_model_{}d_to_{}h'.format(d_lag, n_pre_hours)\n",
    "\n",
    "model.save(best_model_filepath_2)\n",
    "\n",
    "saved_best_model = tf.keras.models.load_model(best_model_filepath_2)\n",
    "\n",
    "y_test_truth = []\n",
    "y_test_hat = []\n",
    "y_each_turn = {}\n",
    "\n",
    "for i in range(0, d_pre_days * 24, n_pre_hours):\n",
    "    \n",
    "    if i == 0:\n",
    "        \n",
    "        # reframe test inputs from each row\n",
    "        test_reframed_single_sample = test_reframed.iloc[i, :].values.reshape(1, -1)\n",
    "        x_test_single_sample = test_reframed_single_sample[:, :n_obs]\n",
    "        y_test_single_sample = test_reframed_single_sample[:, [-n_features * (n_pre_hours - i) for i in range(n_pre_hours)]]\n",
    "        \n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        x_test_single_sample = x_test_single_sample.reshape((x_test_single_sample.shape[0], n_lag_hours, n_features))\n",
    "        \n",
    "        # predict\n",
    "        y_test_hat_single_sample = saved_best_model.predict(x_test_single_sample)\n",
    "        \n",
    "        # append to the list\n",
    "        y_test_truth.extend(y_test_single_sample)\n",
    "        y_test_hat.extend(y_test_hat_single_sample)\n",
    "        \n",
    "        # store prediction results in a dictionary\n",
    "        y_each_turn[i] = y_test_hat_single_sample\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # reframe test inputs from each row\n",
    "        test_reframed_single_sample = test_reframed.iloc[i, :].values.reshape(1, -1)\n",
    "        x_test_single_sample = test_reframed_single_sample[:, :n_obs]\n",
    "        y_test_single_sample = test_reframed_single_sample[:, [-n_features * (n_pre_hours - i) for i in range(n_pre_hours)]]\n",
    "        \n",
    "        # update test inputs with last turn's prediction results\n",
    "        x_test_single_sample[:, [-(n_features * (n_pre_hours - i + n_pre_hours)) \n",
    "                                 for i in range(n_pre_hours)]] = y_each_turn[i-n_pre_hours]\n",
    "        \n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        x_test_single_sample = x_test_single_sample.reshape((x_test_single_sample.shape[0], n_lag_hours, n_features))\n",
    "        \n",
    "        # predict\n",
    "        y_test_hat_single_sample = saved_best_model.predict(x_test_single_sample)\n",
    "        \n",
    "        # append to the list\n",
    "        y_test_truth.extend(y_test_single_sample)\n",
    "        y_test_hat.extend(y_test_hat_single_sample)\n",
    "        \n",
    "        # store prediction results in a dictionary\n",
    "        y_each_turn[i] = y_test_hat_single_sample\n",
    "\n",
    "#%%\n",
    "# model prediction and invert back for comparison\n",
    "\n",
    "hist, truth, pred = create_Y_triplets(test_reframed, y_test_truth, y_test_hat, n_features, n_lag_hours, 0)\n",
    "\n",
    "y_test_compare = pd.concat([pd.DataFrame(np.expm1(truth)), \n",
    "                            pd.DataFrame(np.expm1(pred))], axis=1)\n",
    "\n",
    "y_test_compare.columns = ['truth', 'predicted']\n",
    "\n",
    "y_test_compare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the model perform?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle_code()\n",
    "\n",
    "#%%\n",
    "# model evaluation\n",
    "rmse, cv_rmse = model_score_rmse(y_test_compare.iloc[:, :])\n",
    "\n",
    "print('model score (RMSE) is: \\n', \"{:.2f}\".format(rmse))\n",
    "\n",
    "print('model score (CV_RMSE %) is: \\n', \"{:.2f}\".format(cv_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the baseline model perform?**\n",
    "\n",
    "The baseline model is the simple average of similar days in the past 3 weeks. You can modify parameters to tune your own baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toggle_code()\n",
    "\n",
    "hist_reps = 3\n",
    "\n",
    "baseline = baseline_predict(train_scaled, test_scaled, n_lag_hours, d_pre_days, hist_reps)\n",
    "\n",
    "y_baseline_compare = pd.concat([pd.DataFrame(np.expm1(truth)), \n",
    "                            pd.DataFrame(np.expm1(baseline))], axis=1)\n",
    "\n",
    "y_baseline_compare.columns = ['truth', 'baseline']\n",
    "\n",
    "#%%\n",
    "# baseline evaluation\n",
    "baseline_rmse, baseline_cv_rmse = model_score_rmse(y_baseline_compare.iloc[:, :])\n",
    "\n",
    "print('baseline model score (RMSE) is: \\n', \"{:.2f}\".format(baseline_rmse))\n",
    "\n",
    "print('baseline model score (CV_RMSE %) is: \\n', \"{:.2f}\".format(baseline_cv_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, we plot all results together.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "toggle_code()\n",
    "\n",
    "# reg vs true profile comparison\n",
    "\n",
    "multi_step_plot(hist, truth, pred, baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
